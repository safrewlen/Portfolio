{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import fasttext\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'tn.json', 'r', encoding=\"utf8\") as f:\n",
    "    data_tn = json.loads(f.read())\n",
    "\n",
    "with open(r'tp.json', 'r', encoding=\"utf8\") as f:\n",
    "    data_tp = json.loads(f.read())\n",
    "    \n",
    "df_tp = pd.DataFrame(data_tp, columns=['sentences'])\n",
    "df_tp['event'] = np.ones(len(data_tp)).astype(int)\n",
    "\n",
    "df_tn = pd.DataFrame(data_tn, columns=['sentences'])\n",
    "df_tn['event'] = np.zeros(len(data_tn)).astype(int)\n",
    "\n",
    "train_df = pd.concat([df_tn, df_tp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.sentences, train_df.event, test_size=0.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1335,), (1669, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1340\n",
       "1     329\n",
       "Name: event, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.event.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень мало данных для обучения("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "def preprocessing(data):\n",
    "    a = data.copy()\n",
    "    for i in tqdm(a.index):\n",
    "        # remove special characters\n",
    "        a[i] = re.sub(r'\\W', ' ', a[i])\n",
    "        #remove numbers\n",
    "        a[i] = re.sub(r'\\d', '', a[i].lower())\n",
    "        # Substituting multiple spaces with single space\n",
    "        a[i] = re.sub(r'\\s+', ' ', a[i], flags=re.I)\n",
    "\n",
    "        a[i] = m.lemmatize(a[i])\n",
    "        a[i] = ''.join(a[i])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ru = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_prep = preprocessing(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_prep.to_pickle('lem_prep_train_df.pickle')\n",
    "X_train_prep = pd.read_pickle('lem_prep_train_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_prep = preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_prep.to_pickle('lem_prep_test_df.pickle')\n",
    "X_test_prep = pd.read_pickle('lem_prep_test_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vect(X_train, X_test, stopwords = None):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    X_train_tfidf = pd.DataFrame.sparse.from_spmatrix(X_train_tfidf, columns=vectorizer.get_feature_names())\n",
    "    X_test_tfidf = pd.DataFrame.sparse.from_spmatrix(X_test_tfidf, columns=vectorizer.get_feature_names())\n",
    "    return X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vect(X_train, X_test, stopwords = None, ngram = (1, 1)):\n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords, ngram_range=ngram)\n",
    "    X_train_count = vectorizer.fit_transform(X_train)\n",
    "    X_test_count = vectorizer.transform(X_test)\n",
    "    X_train_count = pd.DataFrame.sparse.from_spmatrix(X_train_count, columns=vectorizer.get_feature_names())\n",
    "    X_test_count = pd.DataFrame.sparse.from_spmatrix(X_test_count, columns=vectorizer.get_feature_names())\n",
    "    return X_train_count, X_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498aaa1159134096a35079d86daea36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d199e24148c0408baceaa4e346abacbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=334), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\", ignore_stopwords=True)\n",
    "def do_stemming(data):\n",
    "    data_stem = data.copy()\n",
    "    for i in tqdm(data_stem.index):\n",
    "        data_stem[i] = re.sub(r'\\W', ' ', data_stem[i])\n",
    "        data_stem[i] = re.sub(r'\\d', '', data_stem[i].lower())\n",
    "        text = []\n",
    "        tokens = [token for token in word_tokenize(data_stem[i]) if token not in stopwords_ru and token.strip() not in string.punctuation+'«—»']\n",
    "        for token in tokens:\n",
    "            text.append(stemmer.stem(token))\n",
    "        data_stem[i] = ' '.join(text)\n",
    "    return data_stem\n",
    "\n",
    "X_train_stem = do_stemming(X_train)\n",
    "X_test_stem = do_stemming(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVect + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.547008547008547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train, X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87368421, 0.65306122]),\n",
       " array([0.93609023, 0.47058824]),\n",
       " array([0.90381125, 0.54700855]),\n",
       " array([266,  68], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec + nltk stopwords + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5087719298245614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train, X_test, stopwords=stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + CountVec(with nltk stopwords) + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5932203389830508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (with nltk stopwords) + TF-IDF Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39583333333333337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + TF-IDF Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41237113402061853"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6050420168067226"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec + CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5739130434782609"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = CatBoostClassifier(verbose=False)\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + Count Vec + CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6140350877192983"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = CatBoostClassifier(verbose=False)\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.870217823971694"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_proba = model.predict_proba(X_test_count)[:, 1]\n",
    "\n",
    "roc_auc_score(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec (n_gram) + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5999999999999999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru, (1, 2))\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname import varname\n",
    "# models = [('logreg', LogisticRegression()), ('catboost', CatBoostClassifier(verbose=False))]\n",
    "# word_proc = [('stem', X_train_stem, X_test_stem), ('lem', X_train_prep, X_test_prep)]\n",
    "# vector = [('count', X_train_count)]\n",
    "# for name, model in models:\n",
    "#     print(name)\n",
    "# https://www.codeastar.com/choose-machine-learning-models-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для обучения эмбедингов будем использовать данные из тестовой выборки тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'test_data.json', 'r', encoding=\"utf8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "test_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf95f113467494081c42844d18cc654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ind = test_df.loc[test_df.duplicated('title', keep = 'first'), 'title'].index\n",
    "for i in tqdm(test_df.index):\n",
    "    if i in ind:\n",
    "        test_df.loc[i, 'new_title'] = test_df.loc[i, 'title'] + str(i)\n",
    "    else: test_df.loc[i, 'new_title'] = test_df.loc[i, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afac54e36fb40699bafa74e0fc5a84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_df = pd.DataFrame()\n",
    "for i in tqdm(test_df.index):\n",
    "    test_sent = nltk.tokenize.sent_tokenize(test_df.text[i], 'russian')\n",
    "    test_new_title = [test_df.new_title[i]]*len(test_sent)\n",
    "    new_test_dict = {'title': test_new_title, 'sentences': test_sent}\n",
    "    a = pd.DataFrame(new_test_dict)\n",
    "    new_test_df = new_test_df.append(a, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test_df['stem_sent'] = do_stemming(new_test_df.sentences)\n",
    "# new_test_df.to_pickle('stem_new_test_data.pickle')\n",
    "new_test_df = pd.read_pickle('stem_new_test_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentences</th>\n",
       "      <th>stem_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>За 10 месяцев 2019 года отгружено более 12 тыс...</td>\n",
       "      <td>месяц год отгруж тыс единиц продукц</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>\"Мы побили рекорд 2013 года.</td>\n",
       "      <td>поб рекорд год</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>Тогда за весь год мы отгрузили почти 11,2 тыс....</td>\n",
       "      <td>ве год отгруз тыс штук локомотивн ос</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Отключение Ирана от интернета выявило сильные ...</td>\n",
       "      <td>Рост тарифов на бензин стал таким вынужденным ...</td>\n",
       "      <td>рост тариф бензин стал так вынужден шаг</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Отключение Ирана от интернета выявило сильные ...</td>\n",
       "      <td>О новых тарифах было объявлено ночью в выходно...</td>\n",
       "      <td>нов тариф объявл ноч выходн ден ноябр</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165150</td>\n",
       "      <td>Энергия старта: участники программы выступили ...</td>\n",
       "      <td>\"Я в ''Океане'' уже в четвертый раз, мне очень...</td>\n",
       "      <td>океан четверт очен понрав эт смен</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165151</td>\n",
       "      <td>Энергия старта: участники программы выступили ...</td>\n",
       "      <td>До этого я никогда раньше не имела дела с отра...</td>\n",
       "      <td>раньш имел дел отрасл энергетик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165153</td>\n",
       "      <td>Энергия старта: участники программы выступили ...</td>\n",
       "      <td>По итогам форума эксперты определили лучшие пр...</td>\n",
       "      <td>итог форум эксперт определ лучш проект</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165154</td>\n",
       "      <td>Энергия старта: участники программы выступили ...</td>\n",
       "      <td>Победители и призёры были отмечены на дружинно...</td>\n",
       "      <td>победител призер отмеч дружин церемон награжден</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165155</td>\n",
       "      <td>Энергия старта: участники программы выступили ...</td>\n",
       "      <td>Больше фотографий с энергофорума ожно посмотре...</td>\n",
       "      <td>фотограф энергофорум ожн посмотрет ссылк</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45941 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "2            Уралкуз” изготовил рекордное количество осей   \n",
       "3            Уралкуз” изготовил рекордное количество осей   \n",
       "4            Уралкуз” изготовил рекордное количество осей   \n",
       "20      Отключение Ирана от интернета выявило сильные ...   \n",
       "25      Отключение Ирана от интернета выявило сильные ...   \n",
       "...                                                   ...   \n",
       "165150  Энергия старта: участники программы выступили ...   \n",
       "165151  Энергия старта: участники программы выступили ...   \n",
       "165153  Энергия старта: участники программы выступили ...   \n",
       "165154  Энергия старта: участники программы выступили ...   \n",
       "165155  Энергия старта: участники программы выступили ...   \n",
       "\n",
       "                                                sentences  \\\n",
       "2       За 10 месяцев 2019 года отгружено более 12 тыс...   \n",
       "3                            \"Мы побили рекорд 2013 года.   \n",
       "4       Тогда за весь год мы отгрузили почти 11,2 тыс....   \n",
       "20      Рост тарифов на бензин стал таким вынужденным ...   \n",
       "25      О новых тарифах было объявлено ночью в выходно...   \n",
       "...                                                   ...   \n",
       "165150  \"Я в ''Океане'' уже в четвертый раз, мне очень...   \n",
       "165151  До этого я никогда раньше не имела дела с отра...   \n",
       "165153  По итогам форума эксперты определили лучшие пр...   \n",
       "165154  Победители и призёры были отмечены на дружинно...   \n",
       "165155  Больше фотографий с энергофорума ожно посмотре...   \n",
       "\n",
       "                                              stem_sent  \n",
       "2                   месяц год отгруж тыс единиц продукц  \n",
       "3                                        поб рекорд год  \n",
       "4                  ве год отгруз тыс штук локомотивн ос  \n",
       "20              рост тариф бензин стал так вынужден шаг  \n",
       "25                нов тариф объявл ноч выходн ден ноябр  \n",
       "...                                                 ...  \n",
       "165150                океан четверт очен понрав эт смен  \n",
       "165151                  раньш имел дел отрасл энергетик  \n",
       "165153           итог форум эксперт определ лучш проект  \n",
       "165154  победител призер отмеч дружин церемон награжден  \n",
       "165155         фотограф энергофорум ожн посмотрет ссылк  \n",
       "\n",
       "[45941 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_df[new_test_df.stem_sent.str.len() < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         па уральск кузниц вход групп мечел итог месяц ...\n",
       "1         уралкуз октябр год постав абсолютн рекорд исто...\n",
       "2                       месяц год отгруж тыс единиц продукц\n",
       "3                                            поб рекорд год\n",
       "4                      ве год отгруз тыс штук локомотивн ос\n",
       "                                ...                        \n",
       "165151                      раньш имел дел отрасл энергетик\n",
       "165152    эт заинтересова прекрасн куратор дума одн сам ...\n",
       "165153               итог форум эксперт определ лучш проект\n",
       "165154      победител призер отмеч дружин церемон награжден\n",
       "165155             фотограф энергофорум ожн посмотрет ссылк\n",
       "Name: sentences, Length: 165156, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new_test_df.stem_sent.copy()\n",
    "data.rename('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, X_train_stem, X_test_stem], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('data.txt', sep='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-10c7ef6e2ab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from gensim.models.wrappers import FastText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# model = fasttext.train_unsupervised('data.txt', model='cbow', dim=300, thread=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# from gensim.models.wrappers import FastText \n",
    "from gensim.models import Word2Vec\n",
    "# model = fasttext.train_unsupervised('data.txt', model='cbow', dim=300, thread=2)\n",
    "model = Word2Vec(data_list, sg=0, size=100, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install genism\n",
    "# !conda install -c anaconda gensim=0.12.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = r'C:\\Users\\safre\\Documents\\Sber_NLP\\tn.json'\n",
    "# print(os.path.basename(path))\n",
    "\n",
    "# path = 'C:\\Users\\safre\\Documents\\Sber_NLP\\tn.json'\n",
    "# path = r''+'C:\\Users\\safre\\Documents\\Sber_NLP\\tn.json'\n",
    "# path\n",
    "# # with open(path, 'r', encoding=\"utf8\") as f:\n",
    "# #     data_tp = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Введите путь к данным')\n",
    "# path = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = u'test_data.json'\n",
    "# with open(path, 'r', encoding=\"utf8\") as f:\n",
    "#     data_test = json.loads(f.read())\n",
    "\n",
    "# data_test = pd.DataFrame(data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
