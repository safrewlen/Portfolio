{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Щербаченко Елена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import fasttext\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяйте путь к 3-м файлам, если это необходимо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные:\n",
    "with open(r'tn.json', 'r', encoding=\"utf8\") as f:\n",
    "    data_tn = json.loads(f.read())\n",
    "\n",
    "with open(r'tp.json', 'r', encoding=\"utf8\") as f:\n",
    "    data_tp = json.loads(f.read())\n",
    "    \n",
    "with open(r'test_data.json', 'r', encoding=\"utf8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "# Создаем метки класса\n",
    "df_tp = pd.DataFrame(data_tp, columns=['sentences'])\n",
    "df_tp['event'] = np.ones(len(data_tp)).astype(int)\n",
    "\n",
    "df_tn = pd.DataFrame(data_tn, columns=['sentences'])\n",
    "df_tn['event'] = np.zeros(len(data_tn)).astype(int)\n",
    "\n",
    "train_df = pd.concat([df_tn, df_tp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим на тренировочную и тестовую часть\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.sentences, train_df.event, test_size=0.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1335,), (334,), (1669, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1340\n",
      "1     329\n",
      "Name: event, dtype: int64 \n",
      " 0    1074\n",
      "1     261\n",
      "Name: event, dtype: int64 \n",
      " 0    266\n",
      "1     68\n",
      "Name: event, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.event.value_counts(), '\\n', y_train.value_counts(), '\\n', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данных для обучения мало. Классы несбалансированные, поэтому в качестве метрики возьмем F меру, beta > 1 т.к. recall нам важнее (т.е. не пропустить актуальную новость)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать несколько способов препроцессинга данных: удаление специальных символов, стоп слов, и чисел, а так же будем выбирать между стеммингом и лемматизацией; Count векторайзером и TF-IDF векторайзером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "def preprocessing(data):\n",
    "    a = data.copy()\n",
    "    for i in tqdm(a.index):\n",
    "        # remove special characters\n",
    "        a[i] = re.sub(r'\\W', ' ', a[i])\n",
    "        #remove numbers\n",
    "        a[i] = re.sub(r'\\d', '', a[i].lower())\n",
    "        # Substituting multiple spaces with single space\n",
    "        a[i] = re.sub(r'\\s+', ' ', a[i], flags=re.I)\n",
    "\n",
    "        a[i] = m.lemmatize(a[i])\n",
    "        a[i] = ''.join(a[i])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ru = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocessing(X_train)\n",
    "# X_train_prep.to_pickle('lem_prep_train_df.pickle')\n",
    "# X_train_prep = pd.read_pickle('lem_prep_train_df.pickle')\n",
    "\n",
    "X_test_prep = preprocessing(X_test)\n",
    "# X_test_prep.to_pickle('lem_prep_test_df.pickle')\n",
    "# X_test_prep = pd.read_pickle('lem_prep_test_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vect(X_train, X_test, stopwords = None):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    X_train_tfidf = pd.DataFrame.sparse.from_spmatrix(X_train_tfidf, columns=vectorizer.get_feature_names())\n",
    "    X_test_tfidf = pd.DataFrame.sparse.from_spmatrix(X_test_tfidf, columns=vectorizer.get_feature_names())\n",
    "    return X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vect(X_train, X_test, stopwords = None, ngram = (1, 1)):\n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords, ngram_range=ngram)\n",
    "    X_train_count = vectorizer.fit_transform(X_train)\n",
    "    X_test_count = vectorizer.transform(X_test)\n",
    "    X_train_count = pd.DataFrame.sparse.from_spmatrix(X_train_count, columns=vectorizer.get_feature_names())\n",
    "    X_test_count = pd.DataFrame.sparse.from_spmatrix(X_test_count, columns=vectorizer.get_feature_names())\n",
    "    return X_train_count, X_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24f162312d4fe297580e2e797b3db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3bee595287447fb3af89c5bf5566b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=334), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\", ignore_stopwords=True)\n",
    "def do_stemming(data):\n",
    "    data_stem = data.copy()\n",
    "    for i in tqdm(data_stem.index):\n",
    "        data_stem[i] = re.sub(r'\\W', ' ', data_stem[i])\n",
    "        data_stem[i] = re.sub(r'\\d', '', data_stem[i].lower())\n",
    "        text = []\n",
    "        tokens = [token for token in word_tokenize(data_stem[i]) if token not in stopwords_ru and token.strip() not in string.punctuation+'«—»']\n",
    "        for token in tokens:\n",
    "            text.append(stemmer.stem(token))\n",
    "        data_stem[i] = ' '.join(text)\n",
    "    return data_stem\n",
    "\n",
    "X_train_stem = do_stemming(X_train)\n",
    "X_test_stem = do_stemming(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVect + LogReg\n",
    "Самая простая модель, бэйзлайн от которого будем отталкиваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49844236760124605"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train, X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec + nltk stopwords + LogReg\n",
    "Добавляем удаление стоп слов внутри метода CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45597484276729566"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train, X_test, stopwords=stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + CountVec(with nltk stopwords) + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5434782608695652"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (with nltk stopwords) + TF-IDF Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3166666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + TF-IDF Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33222591362126247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5572755417956656"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec + CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172413793103449"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru)\n",
    "\n",
    "model = CatBoostClassifier(verbose=False)\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + Count Vec + CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.550314465408805"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "\n",
    "model = CatBoostClassifier(verbose=False)\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + Count Vec (n_gram) + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count, X_test_count = count_vect(X_train_prep, X_test_prep, stopwords_ru, (1, 2))\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем СatBoost и CountVectorizer сработали лучше, поэтому для итоговой модели будем брать их и стемминг. Стемминг выбирается в целях экономии времени, потому что тестовых данных намного больше и лемматизация будет работать долго."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для обучения эмбедингов будем использовать данные из тестовой выборки тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>ПАО \"Уральская кузница\" (входит в Группу \"Мече...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Отключение Ирана от интернета выявило сильные ...</td>\n",
       "      <td>Во второй половине ноября Иран охватили протес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Во Владивостоке количество наружной рекламы до...</td>\n",
       "      <td>Смогут ли навести порядок в беспорядочных рекл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ФУТБОЛ-Месси получил рекордный шестой \"Золотой...</td>\n",
       "      <td>3 дек (Рейтер) - Новый рекорд Лионеля Месси. Н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>«ОРУ под напряжением Луны»: фото амурчанина по...</td>\n",
       "      <td>Фотография амурчанина заняла первое место во в...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       Уралкуз” изготовил рекордное количество осей   \n",
       "1  Отключение Ирана от интернета выявило сильные ...   \n",
       "2  Во Владивостоке количество наружной рекламы до...   \n",
       "3  ФУТБОЛ-Месси получил рекордный шестой \"Золотой...   \n",
       "4  «ОРУ под напряжением Луны»: фото амурчанина по...   \n",
       "\n",
       "                                                text  \n",
       "0  ПАО \"Уральская кузница\" (входит в Группу \"Мече...  \n",
       "1  Во второй половине ноября Иран охватили протес...  \n",
       "2  Смогут ли навести порядок в беспорядочных рекл...  \n",
       "3  3 дек (Рейтер) - Новый рекорд Лионеля Месси. Н...  \n",
       "4  Фотография амурчанина заняла первое место во в...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от обучающего набора данных, те данные, которые нам надо разметить представлены не в виде отдельных предложений, а в виде целых абзацев, поэтому разбиваем абзацы на предложения. У нас есть повторяющиеся названия для нескольких строк, поэтому создаем новые названия с пометками, чтобы не потерять из какого абзаца взято предложение, когда мы их разобьем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe732438fc2471f800f4ca696a5b444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Помечаем совпадающие названия новостей\n",
    "ind = test_df.loc[test_df.duplicated('title', keep = 'first'), 'title'].index\n",
    "for i in tqdm(test_df.index):\n",
    "    if i in ind:\n",
    "        test_df.loc[i, 'new_title'] = test_df.loc[i, 'title'] + str(i)\n",
    "    else: test_df.loc[i, 'new_title'] = test_df.loc[i, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1010dc17393d480ea1d55d4bd3b39bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Разбиваем на предложения, для каждого предложения из одного абзаца дцблируем название абзаца\n",
    "new_test_df = pd.DataFrame()\n",
    "for i in tqdm(test_df.index):\n",
    "    test_sent = nltk.tokenize.sent_tokenize(test_df.text[i], 'russian')\n",
    "    test_new_title = [test_df.new_title[i]]*len(test_sent)\n",
    "    new_test_dict = {'title': test_new_title, 'sentences': test_sent}\n",
    "    a = pd.DataFrame(new_test_dict)\n",
    "    new_test_df = new_test_df.append(a, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e87ce1f8b443fd932ac540b40566dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=165156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_df['stem_sent'] = do_stemming(new_test_df.sentences)\n",
    "new_test_df.to_pickle('stem_new_test_data.pickle')\n",
    "# new_test_df = pd.read_pickle('stem_new_test_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         па уральск кузниц вход групп мечел итог месяц ...\n",
       "1         уралкуз октябр год постав абсолютн рекорд исто...\n",
       "2                       месяц год отгруж тыс единиц продукц\n",
       "3                                            поб рекорд год\n",
       "4                      ве год отгруз тыс штук локомотивн ос\n",
       "                                ...                        \n",
       "166820    стро сборн железобетон котор выпуска липецк дс...\n",
       "166821    министерств просвещен рф должн август подготов...\n",
       "166822    задержк сдач квартир зелен алле месяц привод с...\n",
       "166823    общ площад переселя аварийн квартир составля м...\n",
       "166824    помим роскосмос казан заключ контракт поставк ...\n",
       "Name: sentences, Length: 166825, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new_test_df.stem_sent.copy()\n",
    "data = data.rename('sentences')\n",
    "# Соединяем все данные, которые у нас есть, чтобы посчитать вектора для слов\n",
    "data = pd.concat([data, X_train_stem, X_test_stem], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb3b38e3303425db9afc99f351baba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=166825), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for i in tqdm(data.index):\n",
    "    data[i] = data[i].split(' ')\n",
    "    data_list.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data_list, sg=0, size=100, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + CounVect + WordEmb+ CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делим предложения на слова\n",
    "def split_text_col(col):\n",
    "    split_col = col.copy()\n",
    "    for i in split_col.index:\n",
    "        split_col[i] = col[i].split(' ')\n",
    "    return split_col\n",
    "\n",
    "# для каждого слова находим вектор, суммируем все векторы предложения и нормализуем итоговый\n",
    "def sentence2vec(s):\n",
    "    seq = np.array([model.wv[w] for w in s if w in model.wv.vocab.keys()])\n",
    "    v = seq.sum(axis=0)\n",
    "    v = v / ((v ** 2).sum() + 1e-100) ** 0.5 if seq.shape[0] != 0 else np.ones(100)*1.0/100**0.5\n",
    "    # v / ((v ** 2).sum() + 1e-100) ** 0.5 нормализует вектор так, \n",
    "    # что он не зависит от кол-ва слов в предложении, геометрическая длина вектора = 1\n",
    "    # 100 - размер вектора\n",
    "    return v\n",
    "\n",
    "# Добавляем вектор табличку\n",
    "def col2vec(col_train, col_test):\n",
    "    col_train = split_text_col(col_train)\n",
    "    col_train_vec = np.array([sentence2vec(s) for s in col_train.values]).tolist()\n",
    "    col_train_vec = pd.Series(col_train_vec)\n",
    "    col_train_vec = col_train_vec.rename('vector')\n",
    "    \n",
    "    col_test = split_text_col(col_test)\n",
    "    col_test_vec = np.array([sentence2vec(s) for s in col_test.values]).tolist()\n",
    "    col_test_vec = pd.Series(col_test_vec)\n",
    "    col_test_vec = col_test_vec.rename('vector')\n",
    "    return col_train_vec, col_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stem_vec, X_test_stem_vec = col2vec(X_train_stem, X_test_stem)\n",
    "X_train_stem_count, X_test_stem_count = count_vect(X_train_stem, X_test_stem, stopwords_ru)\n",
    "# Соединяем countvect и word2vec\n",
    "X_train_to_model = pd.concat([X_train_stem_count, X_train_stem_vec], axis=1)\n",
    "X_test_to_model = pd.concat([X_test_stem_count, X_test_stem_vec], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for i in range(1, 101):\n",
    "    vectors.append('vec_{0}'.format(i))\n",
    "\n",
    "X_train_to_model[vectors] = pd.DataFrame(X_train_to_model.vector.values.tolist(), index= X_train_to_model.index)\n",
    "X_train_to_model.drop(['vector'], axis=1, inplace=True)\n",
    "X_test_to_model[vectors] = pd.DataFrame(X_test_to_model.vector.values.tolist(), index= X_test_to_model.index)\n",
    "X_test_to_model.drop(['vector'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5607476635514019"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_cat = CatBoostClassifier(verbose=False)\n",
    "model_cat.fit(X_train_to_model, y_train)\n",
    "y_pred = model_cat.predict(X_test_to_model)\n",
    "\n",
    "fbeta_score(y_test, y_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1335, 837), (334, 837))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_to_model.shape, X_test_to_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692307"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_cat.predict(X_train_to_model)\n",
    "fbeta_score(y_train, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Причем часть котельных была запущена раньше ср...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ООО «БалтИнвестСтрой » взял обязательство рань...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Школа на Южном шоссе на 825 мест также может б...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Школу на Южном шоссе планируется сдать на меся...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Беглов перечислил школы, которые в 2019 году с...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>Бизнесвумен взыскала с «Интеко» 1,5 млн за сры...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>Арбитражный суд Москвы взыскал 1,5 миллиона ру...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>Трутнев раскритиковал срыв сроков сдачи объект...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1667</td>\n",
       "      <td>Глава австрийской OMV Райнер Зеле говорил, что...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1668</td>\n",
       "      <td>Сроки сдачи объекта несколько раз переносились...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1669 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  event\n",
       "0     Причем часть котельных была запущена раньше ср...      0\n",
       "1     ООО «БалтИнвестСтрой » взял обязательство рань...      0\n",
       "2     Школа на Южном шоссе на 825 мест также может б...      0\n",
       "3     Школу на Южном шоссе планируется сдать на меся...      0\n",
       "4     Беглов перечислил школы, которые в 2019 году с...      0\n",
       "...                                                 ...    ...\n",
       "1664  Бизнесвумен взыскала с «Интеко» 1,5 млн за сры...      1\n",
       "1665  Арбитражный суд Москвы взыскал 1,5 миллиона ру...      1\n",
       "1666  Трутнев раскритиковал срыв сроков сдачи объект...      1\n",
       "1667  Глава австрийской OMV Райнер Зеле говорил, что...      1\n",
       "1668  Сроки сдачи объекта несколько раз переносились...      1\n",
       "\n",
       "[1669 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279e5eea9c56491a9a9a74fd0f1a55c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1669), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1669,), (165156,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stem = do_stemming(train_df.sentences)\n",
    "test_stem = new_test_df['stem_sent']\n",
    "train_stem.shape, test_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1669, 881), (165156, 881))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stem_count, test_stem_count = count_vect(train_stem, test_stem, stopwords_ru)\n",
    "train_stem_count.shape, test_stem_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1669,), (165156,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stem_vec, test_stem_vec = col2vec(train_stem, test_stem)\n",
    "train_stem_vec.shape, test_stem_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1669, 882), (165156, 882))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_to_model = pd.concat([train_stem_count, train_stem_vec], axis=1)\n",
    "test_to_model = pd.concat([test_stem_count, test_stem_vec], axis=1)\n",
    "train_to_model.shape, test_to_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1669, 981), (165156, 981))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_to_model[vectors] = pd.DataFrame(train_to_model.vector.values.tolist(), index= train_to_model.index)\n",
    "train_to_model.drop(['vector'], axis=1, inplace=True)\n",
    "test_to_model[vectors] = pd.DataFrame(test_to_model.vector.values.tolist(), index= test_to_model.index)\n",
    "test_to_model.drop(['vector'], axis=1, inplace=True)\n",
    "train_to_model.shape, test_to_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_cat = CatBoostClassifier(verbose=False)\n",
    "model_cat.fit(train_to_model, train_df.event)\n",
    "y_pred = model_cat.predict(test_to_model).astype('int')\n",
    "y_proba = model_cat.predict_proba(test_to_model)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'y_pred': y_pred, 'y_proba': y_proba}, index=test_to_model.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentences</th>\n",
       "      <th>stem_sent</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>ПАО \"Уральская кузница\" (входит в Группу \"Мече...</td>\n",
       "      <td>па уральск кузниц вход групп мечел итог месяц ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>\"Уралкуз\" в октябре 2019 года поставил абсолют...</td>\n",
       "      <td>уралкуз октябр год постав абсолютн рекорд исто...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>За 10 месяцев 2019 года отгружено более 12 тыс...</td>\n",
       "      <td>месяц год отгруж тыс единиц продукц</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>\"Мы побили рекорд 2013 года.</td>\n",
       "      <td>поб рекорд год</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Уралкуз” изготовил рекордное количество осей</td>\n",
       "      <td>Тогда за весь год мы отгрузили почти 11,2 тыс....</td>\n",
       "      <td>ве год отгруз тыс штук локомотивн ос</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0  Уралкуз” изготовил рекордное количество осей   \n",
       "1  Уралкуз” изготовил рекордное количество осей   \n",
       "2  Уралкуз” изготовил рекордное количество осей   \n",
       "3  Уралкуз” изготовил рекордное количество осей   \n",
       "4  Уралкуз” изготовил рекордное количество осей   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  ПАО \"Уральская кузница\" (входит в Группу \"Мече...   \n",
       "1  \"Уралкуз\" в октябре 2019 года поставил абсолют...   \n",
       "2  За 10 месяцев 2019 года отгружено более 12 тыс...   \n",
       "3                       \"Мы побили рекорд 2013 года.   \n",
       "4  Тогда за весь год мы отгрузили почти 11,2 тыс....   \n",
       "\n",
       "                                           stem_sent  y_pred   y_proba  \n",
       "0  па уральск кузниц вход групп мечел итог месяц ...       0  0.020999  \n",
       "1  уралкуз октябр год постав абсолютн рекорд исто...       0  0.064358  \n",
       "2                месяц год отгруж тыс единиц продукц       0  0.016204  \n",
       "3                                     поб рекорд год       0  0.011726  \n",
       "4               ве год отгруз тыс штук локомотивн ос       0  0.011136  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([new_test_df, results], axis=1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d95a1768614970a3e469b9eb7e0af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# возвращаемся к исходным данным и каждому абзацу, \n",
    "# в котором есть хоть одно предложение с искомым классом ставим класс 1, \n",
    "# там расчитываем среднюю вероятность принадлежности классу 1, для ранжирования.\n",
    "test_df['y_pred'] = 0\n",
    "test_df['y_proba'] = 0\n",
    "for tit in tqdm(result.title.unique()):\n",
    "    a = result[result.title == tit]\n",
    "    if a.y_pred.any() == True:\n",
    "        test_df.loc[test_df.new_title == tit, 'y_pred'] = 1\n",
    "        test_df.loc[test_df.new_title == tit, 'y_proba'] = a.y_proba.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event = test_df[test_df.y_pred == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event = test_event.sort_values(by='y_proba', ascending=False)[['title', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event.to_json('result.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что можно сделать еще\n",
    "Первоочередное - разобраться с переобучением\n",
    "\n",
    "* Настроить гипер параметры модели,в том числе регуляризацию, потому что я переобучилась\n",
    "* Подать CatBoost валидационный набор данных тоже в целях контроля переобучения\n",
    "* Поварьировать порог классификации\n",
    "* Использовать предобученные векторы слов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
